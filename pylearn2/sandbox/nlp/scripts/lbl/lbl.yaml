# This YAML file is an example of a simple language model which can be trained
# on the Penn Treebank data. It uses a projection layer to embed words in a
# 128-dimensional space.
#

!obj:pylearn2.train.Train {
    dataset: &train !obj:pylearn2.sandbox.nlp.datasets.penntree.PennTreebank {
        which_set: 'train',
        context_len: &context_len 6
    },
    model: !obj:pylearn2.sandbox.nlp.models.vlbl.vLBLSoft {
        dict_size: 10000,
        dim: 500,
        context_length: *context_len,
        k: 50,
        irange: 0.005
    },
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        batch_size: 100,
        learning_rate: 0.5,
        learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
                init_momentum: 0.5,
        },
        cost: !obj:pylearn2.costs.cost.SumOfCosts { costs: [
            !obj:pylearn2.sandbox.nlp.models.lblcost.Default {},
            !obj:pylearn2.sandbox.nlp.models.lblcost.Dropout {
                #coeffs: [ .00005, .00005, .00005 ]
                default_input_include_prob: 0.5,
                default_input_scale: 5.
            }
            ]
        },
        monitoring_dataset: {
            'valid' : !obj:pylearn2.sandbox.nlp.datasets.penntree.PennTreebank {
                which_set: 'valid',
                context_len: *context_len
            },
            'train' : *train,
        },
        termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {
            channel_name: 'valid_perplexity',
            prop_decrease: 0.,
            N: 10
        },
    },

}
